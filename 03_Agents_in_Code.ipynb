{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c140625-5760-4543-a9dd-28020ffed382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Authoring Agents in Code\n",
    "\n",
    "If you need the additional flexibility and control over your agents, we recommend developing agents in code. At Databricks, we are a framework agnostic platform so you can bring any framework to orchestrate against your LLM. \n",
    "\n",
    "For this notebook, we will be using DSPy to quickly define tools and deploy agent endpoints. \n",
    "\n",
    "First, we will define the tools we want our Agent to use. Then, we will use DSPy to create a signature that DSPy will compile into a Prompt to send to the LLM. We will test this Agent, then deploy on Databricks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a9f254-d9ba-41fe-9c36-92e8de93f7a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade dspy==3.0.0b3 mlflow unitycatalog-ai[databricks] databricks-sdk databricks-vectorsearch databricks-agents git+https://github.com/BerriAI/litellm.git@main\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98905c89-fd50-422b-9205-e6637d3124dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 1: Define your Tools\n",
    "\n",
    "As you saw in the past notebooks, it's critical that we provide tools to our agents so that they become data intelligent and can access information they may not have access to. \n",
    "\n",
    "Let's use the Vector Search Endpoint created by the Knowledge Assistant Agent Brick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d70cf50-906b-45ad-9935-53e8a0f11384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint_name= \"ka-01481fed-endpoint\" #change this to your agent bricks endpoint\n",
    "\n",
    "dbutils.widgets.text(\"endpoint_name\", endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8115d38f-ca8b-4032-8c96-6207f3977fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.deployments\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def sec_search(sec_question):\n",
    "    \"\"\"This function takes a user's question and returns relevant information from companies' 10-K, 8-K, and annual reports.\"\"\"\n",
    "    client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    response = client.predict(\n",
    "        endpoint=dbutils.widgets.get(\"endpoint_name\"),\n",
    "        inputs={\"dataframe_split\": {\n",
    "            \"columns\": [\"input\"],\n",
    "            \"data\": [[\n",
    "                [{\"role\": \"user\", \"content\": sec_question}]\n",
    "            ]]\n",
    "        }}\n",
    "    )\n",
    "    return response['predictions']['output'][0]['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e9f20a-6868-4d24-9356-d2247743606f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = sec_search(\"Who is Michael (Mike) Mohan?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a96e186-9477-48ce-ad67-4feb9ed93107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To keep the workshop simple, we will only use one tool and keep this as a simple RAG agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ff1562-7487-42aa-8d5d-6516fe6f4309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 2: Define the DSPy Signature \n",
    "\n",
    "Now we need to define the DSPy signature. This signature is explain what we are trying to accomplish and define the inputs and outputs for the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4d564f-079a-422a-8e9d-9fdf0247c580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import mlflow\n",
    "llm = dspy.LM('databricks/databricks-llama-4-maverick', cache=False)\n",
    "# llm = dspy.LM('databricks/databricks-meta-llama-3-1-8b-instruct', cache=False)\n",
    "# llm = dspy.LM('databricks/databricks-claude-3-7-sonnet', cache=False)\n",
    "dspy.configure(lm=llm)\n",
    "mlflow.dspy.autolog()\n",
    "# mlflow.set_experiment(experiment_id=\"835bf9ec05f24eb09289e8030853d968\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf5abba-bd78-4718-836e-cc015784ec72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import Literal\n",
    "class rag_signature(dspy.Signature):\n",
    "  \"\"\"\n",
    "  use the given tools to answer the question\n",
    "  \"\"\" \n",
    "  question: str = dspy.InputField()\n",
    "  response: str = dspy.OutputField() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16ca2e60-1b0c-4e76-ab4e-8db8d16c1ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Step 3: Test the Signature\n",
    "\n",
    "Let's see if we can get the LLM to execute the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143b1b0d-b16d-4634-b8e7-187ac5457498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rag = dspy.ReAct(rag_signature, tools=[sec_search], max_iters=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1afe9bf1-dae7-42f2-ae2d-7a10eeecc132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"Who is Michael (Mike) Mohan?\"\n",
    "result = rag(question=question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419ad9e1-a14d-413d-a209-dc26240a33b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The response: {result.response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5a2527-b3a2-4f73-b74b-47a8f2ff553e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Call out: MLflow Traces\n",
    "\n",
    "Mlflow traces gives us additional visibility into what is happening under the hood. As you develop more complex Agents, it will be difficult to see what is happening or what decisions the LLM makes. Sometimes, your agent will finish executing even if the tool fails to complete but you will have no idea this happened. It's important to add tracing to you agent development workflow or else you will be working in the dark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d2ef96a-8d46-4aa9-b0a2-b844634cc351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Let's Deploy this! \n",
    "\n",
    "At Databricks, we deploy our agents in code using the MLflow ChatAgent object. This allows us to use agents.deploy that creates an agent endpoint complete with a feedback model and review app for feedback from our Subject Matter Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4265df46-e82e-4b11-b00b-61f8ea42b7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "\n",
    "from typing import Any, Generator, Optional\n",
    "import mlflow\n",
    "import mlflow.deployments\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc.model import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "import dspy\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "\n",
    "mlflow.dspy.autolog()\n",
    "mlflow.set_experiment(experiment_id=\"835bf9ec05f24eb09289e8030853d968\")\n",
    "LLM_ENDPOINT_NAME = \"databricks-llama-4-maverick\"\n",
    "lm = dspy.LM(model=f\"databricks/{LLM_ENDPOINT_NAME}\")\n",
    "dspy.settings.configure(lm=lm)\n",
    "endpoint_name= \"ka-ca0f678f-endpoint\" #update this\n",
    "\n",
    "class rag_signature(dspy.Signature):\n",
    "  \"\"\"\n",
    "  use the given tools to answer the question\n",
    "  \"\"\" \n",
    "  question: str = dspy.InputField()\n",
    "  response: str = dspy.OutputField() \n",
    "\n",
    "class DSPyChatAgent(ChatAgent):     \n",
    "    def __init__(self):\n",
    "      self.rag_signature = rag_signature\n",
    "      self.endpoint_name = endpoint_name\n",
    "      self.rag_agent = dspy.ReAct(self.rag_signature, tools=[self.sec_search],max_iters=1)\n",
    "    \n",
    "\n",
    "    def sec_search(self, databricks_question):\n",
    "        \"\"\"This function needs the User's question. The question is used to pull documentation about Databricks. Use the information to answer the user's question\"\"\"\n",
    "        client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "        response = client.predict(\n",
    "            endpoint=self.endpoint_name,\n",
    "            inputs={\"dataframe_split\": {\n",
    "                \"columns\": [\"input\"],\n",
    "                \"data\": [[\n",
    "                    [{\"role\": \"user\", \"content\": databricks_question}]\n",
    "                ]]\n",
    "            }}\n",
    "        )\n",
    "        return response['predictions']['output'][0]['content'][0]['text']\n",
    "      \n",
    "    def prepare_message_history(self, messages: list[ChatAgentMessage]):\n",
    "        history_entries = []\n",
    "        # Assume the last message in the input is the most recent user question.\n",
    "        for i in range(0, len(messages) - 1, 2):\n",
    "            history_entries.append({\"question\": messages[i].content, \"answer\": messages[i + 1].content})\n",
    "        return dspy.History(messages=history_entries)\n",
    "      \n",
    "    @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        latest_question = messages[-1].content\n",
    "        response = self.rag_agent(question=latest_question).response\n",
    "        return ChatAgentResponse(\n",
    "            messages=[ChatAgentMessage(role=\"assistant\", content=response, id=uuid.uuid4().hex)]\n",
    "        )\n",
    "\n",
    "from mlflow.models import set_model\n",
    "AGENT = DSPyChatAgent()\n",
    "set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ccb9c3-c5fc-466b-a34d-691da3398abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c4c2e99-e2c6-49a8-bbf0-48b9ca154f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"What were the key financial highlights for Amazon in 2019?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0463ba2a-ea67-4838-8815-5aaf967a392a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Let's evaluate our agent\n",
    "\n",
    "One of the most powerful parts of MLflow 3.0 is the evaluation frameworks. You can track all your evaluation runs and compare them in the MLflow Experiments page. All activity such as traces, judges and more can be found in that experiment. \n",
    "\n",
    "You can work with your agents.py file to update the agent if you find it not performing to your expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d805935f-a875-4742-b8f4-e09c9edf2e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai import datasets, evaluate, scorers\n",
    "from agent import AGENT\n",
    "from mlflow.genai.scorers import (\n",
    "    RetrievalGroundedness,\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    "    Guidelines,\n",
    ")\n",
    "\n",
    "# You can manually set an experiment to organize your traces and evaluation runs under one experiment. However, mlflow.autolog will automatically create one for you to use associated with this notebook. \n",
    "\n",
    "# mlflow.set_experiment(experiment_id=\"835bf9ec05f24eb09289e8030853d968\")\n",
    "\n",
    "#A dummy question set to run our evaluations against\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What were the key financial highlights for Amazon in 2019?\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who was the CEO of Nike in 2018?\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What strategic initiatives did Adobe announce in their 2018 reports?\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What were Walmart's sustainability goals mentioned in their 2020 documents?\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "#You can set your own judges or scorers here! Guidelines are a quick way to prompt how you want your LLM as a judge to evaluate the responses.\n",
    "bricks_scorers = [\n",
    "  Guidelines(\n",
    "    name=\"relevant_content\",\n",
    "    guidelines=\"The generated response is similar to the retrieved information\",\n",
    "  ),\n",
    "  Guidelines(\n",
    "    name=\"professional\",\n",
    "    guidelines=\"the generated response sounds professional and polite\",\n",
    "  ) ,\n",
    "  Guidelines(\n",
    "    name=\"concise\",\n",
    "    guidelines=\"the generated response is concise and to the point\",\n",
    "  ),\n",
    "  Guidelines(\n",
    "    name=\"no_personal_opinion\",\n",
    "    guidelines=\"the generated response does not contain any personal opinion or bias\",\n",
    "  ),\n",
    "  RelevanceToQuery(),\n",
    "  Safety()\n",
    "]\n",
    "\n",
    "def predict(question):\n",
    "  result = AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": question}]})\n",
    "  return {\"response\": result.messages[0].content}\n",
    "\n",
    "# Step 3: Run evaluation\n",
    "evaluate(\n",
    "  data=eval_dataset,\n",
    "  predict_fn=predict,\n",
    "  scorers=bricks_scorers\n",
    ")\n",
    "\n",
    "# Results will appear back in this UI, you can click the button below to review the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d90152-048c-4f28-b3ac-a0eef5ae0df7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksVectorSearchIndex,\n",
    "    DatabricksServingEndpoint,\n",
    ")\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=\"databricks-llama-4-maverick\"),\n",
    "    DatabricksServingEndpoint(endpoint_name=dbutils.widgets.get(\"endpoint_name\")),\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        # input_example=input_example,\n",
    "        extra_pip_requirements=[f\"databricks-connect=={get_distribution('databricks-connect').version}\"],\n",
    "        resources=resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbd3dbf-7488-47eb-babc-e49b9897490e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "email = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "email = email.split(\"@\")[0].replace(\".\",\"_\")\n",
    "\n",
    "catalog = \"genai_in_production_demo_catalog\"\n",
    "schema = \"agents\"\n",
    "model_name = f\"ai_pioneer_agent_{email}\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "print(f\"Your model name: {UC_MODEL_NAME}\")\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9c4307-fd87-48b4-a606-151095853024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Now let's deploy it!\n",
    "\n",
    "###*NOTE* \n",
    "Unfortunately, due to the limitations of the trial workspace, we are not able to deploy more than one agent endpoint. So please refer to the presenters to demonstrate the agent endpoint and review app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1efe7e7d-2efe-4c1b-95a4-4eea1564a2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import os\n",
    "\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, environment_vars={\"MLFLOW_TRACKING_URI\": \"databricks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f2393dc-c304-47f8-9667-8c756772613e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Congrats! You have an Agent! \n",
    "\n",
    "Check out the links above to see your agents deploy!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53455ed6-21da-4a60-9719-089ef5af12ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#But we aren't done! \n",
    "\n",
    "Make sure to Evaluate your agent before and after! Having good evaluations grounded in your metrics to monitor for any diverging behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432e0819-0f10-4134-b0a0-3e5e6aabb74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai import evaluate, scorers\n",
    "import mlflow.genai.datasets\n",
    "from agent import AGENT\n",
    "from mlflow.genai.scorers import (\n",
    "    RetrievalGroundedness,\n",
    "    RelevanceToQuery,\n",
    "    Safety,\n",
    "    Guidelines,\n",
    ")\n",
    "\n",
    "# mlflow.set_experiment(experiment_id=\"835bf9ec05f24eb09289e8030853d968\") #use this if you want to use more traces or preloaded traces\n",
    "traces = mlflow.search_traces(\n",
    "  max_results=5,\n",
    "  order_by=[\"timestamp DESC\"]\n",
    ")\n",
    "\n",
    "bricks_scorers = [\n",
    "  Guidelines(\n",
    "    name=\"relevant_content\",\n",
    "    guidelines=\"The generated response is relevant to the retrieved information\",\n",
    "  ),\n",
    "  Guidelines(\n",
    "    name=\"professional\",\n",
    "    guidelines=\"the generated response sounds professional and polite\",\n",
    "  ) ,\n",
    "  Guidelines(\n",
    "    name=\"concise\",\n",
    "    guidelines=\"the generated response is concise and to the point\",\n",
    "  ),\n",
    "  Guidelines(\n",
    "    name=\"no_personal_opinion\",\n",
    "    guidelines=\"the generated response does not contain any personal opinion or bias\",\n",
    "  ),\n",
    "  RelevanceToQuery(),\n",
    "  Safety()\n",
    "]\n",
    "\n",
    "# Step 3: Run evaluation\n",
    "evaluate(\n",
    "  data=traces,\n",
    "  scorers=bricks_scorers\n",
    ")\n",
    "\n",
    "# Results will appear back in this UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4906ff99-b0ab-4e0d-9b90-551d61609f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5763010650974512,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "vector_endpoint",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "vector_index",
      "width": 172
     }
    ]
   },
   "notebookName": "03_Agents_in_Code",
   "widgets": {
    "endpoint_name": {
     "currentValue": "ka-ca0f678f-endpoint",
     "nuid": "b47a92ac-9c45-4e6c-ba3d-b3940487531d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ka-01481fed-endpoint",
      "label": null,
      "name": "endpoint_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ka-01481fed-endpoint",
      "label": null,
      "name": "endpoint_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
